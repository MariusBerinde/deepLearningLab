{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"pseAhGTKdzJx"},"outputs":[],"source":["import torch"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"amOEQIUudzJz"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'sklearn'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_regression\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"]}],"source":["from sklearn.datasets import make_regression\n","import matplotlib.pyplot as plt\n","\n","bias = 0\n","num_features = 1\n","X_numpy, y_numpy, coef = make_regression(\n","    n_samples=5000,\n","    n_features=num_features,\n","    n_targets=1,\n","    noise=5,\n","    bias=bias,\n","    coef=True,\n","    random_state=42\n",")"]},{"cell_type":"markdown","metadata":{"id":"TbYnDE42dzJ0"},"source":["## Solving the least squares problem via gradient descent"]},{"cell_type":"markdown","metadata":{"id":"Gr43qC5ydzJ1"},"source":["Suppose we are given $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{b} \\in \\mathbb{R}^{m \\times 1}$ and we want to find the vector $\\boldsymbol{x} \\in \\mathbb{R}^{n \\times 1}$ that minimizes $f(\\boldsymbol{x})$:\n","\n","$$\n","    f(\\boldsymbol{x}) = \\frac{1}{2} \\| \\mathbf{A}\\boldsymbol{x} -b \\|_2^2\n","$$"]},{"cell_type":"markdown","metadata":{"id":"f1piaLJFdzJ1"},"source":["Note that if we set $\\mathbf{A}$ to be an encoding of a training set, $\\mathbf{b}$ to be an encoding of the target value, then $\\boldsymbol{x}$ can be thought of as the parameters of a linear model that approximates the target value.\n","\n","There are specialized linear algebra algorithms that can solve this problem efficiently. However, we can also explore how to solve it using gradient-based optimization as a simple example of how these techniques work.\n","\n","The gradient of $f$ is:\n","\n","$$\n","    \\nabla_\\boldsymbol{x} f(\\boldsymbol{x}) = \\mathbf{A}^\\top (\\mathbf{A} \\boldsymbol{x} - \\mathbf{b}) = \\mathbf{A}^\\top \\mathbf{A} \\boldsymbol{x} - \\mathbf{A}^\\top \\mathbf{b}\n","$$\n","\n","To see why this is so, please recall that $\\| \\boldsymbol{v} \\|^2_2 = \\boldsymbol{v}^\\top \\boldsymbol{v}$, yielding:\n","\n","$$\n","\\begin{aligned}\n","    \\frac{1}{2} \\| \\mathbf{A}\\boldsymbol{x} - \\mathbf{b} \\|^2_2\n","        & = \\frac{1}{2} (\\mathbf{A}\\boldsymbol{x} - \\mathbf{b})^\\top (\\mathbf{A}\\boldsymbol{x} - \\mathbf{b}) \\\\\n","        & = \\frac{1}{2} (\\boldsymbol{x}^\\top \\mathbf{A}^\\top - \\mathbf{b}^T)(\\mathbf{A}\\boldsymbol{x} - \\mathbf{b}) \\\\\n","        & = \\frac{1}{2} (\\boldsymbol{x}^\\top \\mathbf{A}^\\top \\mathbf{A}\\boldsymbol{x}  - \\boldsymbol{x}^\\top \\mathbf{A}^\\top \\mathbf{b} - \\mathbf{b}^T \\mathbf{A}\\boldsymbol{x} + \\mathbf{b}^T \\mathbf{b})\n","\\end{aligned}\n","$$\n","\n","<!-- what we need:\n","    - (AB).T = B.T A.T\n","    - \\| v \\|^2 = v.T v\n","-->"]},{"cell_type":"markdown","metadata":{"id":"lHB4fMZLdzJ2"},"source":["we have then:\n","\n","$$\n","\\begin{aligned}\n","    \\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x}) & = \\nabla_{\\boldsymbol{x}} \\frac{1}{2} (\\boldsymbol{x}^\\top \\mathbf{A}^\\top \\mathbf{A}\\boldsymbol{x}  - \\boldsymbol{x}^\\top \\mathbf{A}^\\top \\mathbf{b} - \\mathbf{b}^T \\mathbf{A}\\boldsymbol{x} + \\mathbf{b}^T \\mathbf{b}) \\\\\n","    & = \\frac{1}{2} (\\nabla_{\\boldsymbol{x}} \\boldsymbol{x}^\\top \\mathbf{A}^\\top \\mathbf{A}\\boldsymbol{x}  - \\nabla_{\\boldsymbol{x}} \\boldsymbol{x}^\\top \\mathbf{A}^\\top \\mathbf{b} - \\nabla_{\\boldsymbol{x}} \\mathbf{b}^T \\mathbf{A}\\boldsymbol{x} + \\nabla_{\\boldsymbol{x}} \\mathbf{b}^T \\mathbf{b}) \\\\\n","    & = \\frac{1}{2} (2 \\mathbf{A}^\\top \\mathbf{A} \\boldsymbol{x} - \\mathbf{A}^\\top \\mathbf{b} - \\mathbf{A}^T \\mathbf{b}) \\\\\n","    & = \\frac{1}{2} (2 \\mathbf{A}^\\top \\mathbf{A} \\boldsymbol{x} - 2 \\mathbf{A}^\\top \\mathbf{b}) \\\\\n","    & = \\mathbf{A}^\\top \\mathbf{A} \\boldsymbol{x} - \\mathbf{A}^\\top \\mathbf{b}\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"16Pwc9XbX4Qm"},"source":[]},{"cell_type":"markdown","metadata":{"id":"bhZm7DskdzJ3"},"source":["where we used the following identities:\n","\n","$$\n","\\begin{aligned}\n","    \\nabla_{\\boldsymbol{x}} \\boldsymbol{x}^\\top \\mathbf{Z} \\boldsymbol{x} & = 2 \\mathbf{Z} \\boldsymbol{x}, & \\text{which holds if $\\mathbf{Z}$ is symmetric and not a function of $\\boldsymbol{x}$}; \\\\\n","    \\nabla_{\\boldsymbol{x}} \\mathbf{a}^\\top  \\boldsymbol{x} = \\nabla_{\\boldsymbol{x}} \\boldsymbol{x}^T \\mathbf{a} & = \\mathbf{a}, & \\text{which holds if $\\mathbf{a}$ is not a function of $\\boldsymbol{x}$}.\n","\\end{aligned}\n","$$\n","\n","For an introduction to differentiation in matrix form, see https://atmos.uw.edu/~dennis/MatrixCalculus.pdf"]},{"cell_type":"markdown","metadata":{"id":"n9VvcKqKdzJ3"},"source":["We can then follow the gradient downhill taking small steps by iteratively update $\\boldsymbol{x}$ using the rule $\\boldsymbol{x} \\leftarrow \\boldsymbol{x} - \\epsilon \\nabla_{\\boldsymbol{x}} f(\\boldsymbol{x})$ until convergence. Let's implement this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZNRzFqqdzJ3"},"outputs":[],"source":["def f(A,b,x):\n","    \"\"\"\n","    Compute f(x) = 1/2 * ||Ax - b||^2.\n","    \"\"\"\n","    TODO\n","\n","def grad_f(A,b,x):\n","    TODO\n","\n","def minimize_f(A, b, eps, delta):\n","    \"\"\"\n","    Minimize f(x) = 1/2 * ||Ax - b||^2 using gradient descent.\n","    eps: learning rate, used in the update rule: x <- x - eps * grad(f(x))\n","    delta: the algorithm has to stop when the norm of the gradient is less than delta\n","    \"\"\"\n","\n","    TODO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpK8j2IVdzJ4"},"outputs":[],"source":["X = torch.tensor(X_numpy)\n","y = torch.tensor(y_numpy)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PpYDR8yPdzJ4"},"outputs":[],"source":["xopt = minimize_f(X,y,1E-6,1E-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2SQFs1BCdzJ4"},"outputs":[],"source":["def plot_model(X,y,w):\n","    xx = torch.linspace(-3, 3, 100, dtype=torch.float64)\n","    xx = xx.reshape(100,1)\n","    yy = xx @ w\n","    plt.scatter(X, y)\n","    plt.plot(xx, yy, c='red')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bGz76DQxdzJ4"},"outputs":[],"source":["plot_model(X,y,xopt)"]},{"cell_type":"markdown","metadata":{"id":"NjLYf6HfdzJ5"},"source":["\n","## Solving the problem the torch way"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"elapsed":433,"status":"error","timestamp":1695830552361,"user":{"displayName":"Roberto Esposito","userId":"13465379332443461245"},"user_tz":-120},"id":"bzHnBnZ_dzJ5","outputId":"615ce087-c7d6-40d7-8bc6-43be96a2a46f"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-88610b034451>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mwopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_minimize_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1E-6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1E-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"]}],"source":["def torch_minimize_loss(A, b, eps, delta):\n","    \"\"\"\n","    Minimize f(x) = 1/2 * ||Ax - y||^2 using gradient descent. With gradients\n","    computed using autograd.\n","    \"\"\"\n","\n","    TODO\n","\n","\n","wopt = torch_minimize_loss(X,y,1E-6,1E-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZRAHhNB7dzJ5"},"outputs":[],"source":["\n","# wopt is a tensor for which we can compute gradients. matplotlib does not\n","# support plotting tensors that require gradients tracking, so we need to\n","# detach it from the computation graph and convert it to a numpy array.\n","plot_model(X,y,wopt)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"venv.nosync","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
